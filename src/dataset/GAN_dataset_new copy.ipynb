{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook to show the different steps of the code of the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import tqdm  # Ensure tqdm is imported if not already\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the parameters / hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Definition of hyperparameters\n",
    "num_epochs = 100\n",
    "learning_rate_g = 0.0001\n",
    "learning_rate_d = 0.0002\n",
    "latent_vector_size = 40\n",
    "\n",
    "#Â Other hyperparams\n",
    "channel_input = 32\n",
    "channel_output = 3\n",
    "\n",
    "batch_size = 128####REDUNDENT#####\n",
    "latent_dim = 50####REDUNDENT##### # Choose a value for the size of the latent space\n",
    "\n",
    "# Additional Hyperparameters ####REDUNDENT#####\n",
    "beta = 1####REDUNDENT#####\n",
    "\n",
    "content_path = \"\"\n",
    "content_path = Path(content_path)\n",
    "\n",
    "# Necessary Hyperparameters ####REDUNDENT##### --- >  redefined later on for convidence\n",
    "num_epochs = 10####REDUNDENT#####\n",
    "learning_rate = 1e-3####REDUNDENT#####\n",
    "batch_size = 128####REDUNDENT#####\n",
    "latent_dim = 50####REDUNDENT##### # Choose a value for the size of the latent space\n",
    "\n",
    "# Additional Hyperparameters ####REDUNDENT#####\n",
    "beta = 1####REDUNDENT#####\n",
    "\n",
    "def binary_threshold(input_tensor, threshold = 0.5):\n",
    "    # Apply threshold\n",
    "    return (input_tensor > threshold).float()\n",
    "\n",
    "# (Optionally) Modify transformations on input\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(binary_threshold),\n",
    "])\n",
    "\n",
    "# (Optionally) Modify the network's output for visualizing your images\n",
    "def denorm(x):\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data (Need to finish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 42\u001b[0m\n\u001b[1;32m     37\u001b[0m     image_arrays\u001b[38;5;241m.\u001b[39mappend(img_array)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Optionally, print the shape of each image to confirm\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# print(f\"Resized image shape: {img_array.shape}\")  # Should output (128, 128, 3)\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimage_arrays\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)), # Dimensions of the picture, trade-off between quality and computational cost\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    return transform(image)\n",
    "\n",
    "# Define the directory and pattern to match the filenames\n",
    "# Faut bien que dans ton terminal tu sois dans le folder 'French GAN' sinon ca l'accedera pas \n",
    "# C'est un peu chiant mais j'ai pas trouve mieux pour l'instant\n",
    "image_directory = 'Pistachio_Image_Dataset/Kirmizi_Pistachio'\n",
    "\n",
    "pattern = os.path.join(image_directory, 'kirmizi *.jpg')\n",
    "\n",
    "# Use glob to get all the file paths that match the pattern\n",
    "image_paths = glob.glob(pattern)\n",
    "# print(len(image_paths))\n",
    "\n",
    "# Initialize an empty list to store the image arrays\n",
    "image_arrays = []\n",
    "\n",
    "# Loop through the matched image paths\n",
    "for image_path in image_paths:\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Resize the image to 128x128\n",
    "    img_resized = image.resize((128, 128))\n",
    "    \n",
    "    # Convert the image to a NumPy array\n",
    "    img_array = np.array(img_resized)\n",
    "    \n",
    "    # Append the NumPy array to the list\n",
    "    image_arrays.append(img_array)\n",
    "\n",
    "    # Optionally, print the shape of each image to confirm\n",
    "    # print(f\"Resized image shape: {img_array.shape}\")  # Should output (128, 128, 3)\n",
    "\n",
    "print(image_arrays.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model of the Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Define the generator's layers with explicit parameter naming\n",
    "        self.conv_transpose1 = nn.ConvTranspose2d(in_channels=latent_vector_size, out_channels=channel_input * 16, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=channel_input * 16)\n",
    "        self.relu1 = nn.LeakyReLU(inplace=True)\n",
    "        \n",
    "        self.conv_transpose2 = nn.ConvTranspose2d(in_channels=channel_input * 16, out_channels=channel_input * 8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=channel_input * 8)\n",
    "        self.relu2 = nn.LeakyReLU(inplace=True)\n",
    "        \n",
    "        self.conv_transpose3 = nn.ConvTranspose2d(in_channels=channel_input * 8, out_channels=channel_input * 4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(num_features=channel_input * 4)\n",
    "        self.relu3 = nn.LeakyReLU(inplace=True)\n",
    "        \n",
    "        self.conv_transpose4 = nn.ConvTranspose2d(in_channels=channel_input * 4, out_channels= channel_output, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.relu1(self.batch_norm1(self.conv_transpose1(z)))\n",
    "        z = self.relu2(self.batch_norm2(self.conv_transpose2(z)))\n",
    "        z = self.relu3(self.batch_norm3(self.conv_transpose3(z)))\n",
    "        z = self.tanh(self.conv_transpose4(z))\n",
    "        return z\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel_output, channel_input*4, 4, 2, 1, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(channel_input*4)\n",
    "        self.leaky_relu1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(channel_input*4, channel_input*8, 4, 2, 1, bias=False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(channel_input*8)\n",
    "        self.leaky_relu2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channel_input*8, channel_input * 16, 4, 2, 1, bias=False)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(channel_input*16)\n",
    "        self.leaky_relu3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(channel_input*16, channel_input * 32, 4, 2, 1, bias=False)\n",
    "        self.batch_norm4 = nn.BatchNorm2d(channel_input*32)\n",
    "        self.leaky_relu4 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(channel_input*32, channel_input * 64, 2, 1, 1, bias=False)\n",
    "        self.batch_norm5 = nn.BatchNorm2d(channel_input*64)\n",
    "        self.leaky_relu5 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(channel_input*64, 1, 3, 1, 0, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu1(self.batch_norm1(self.conv1(x)))\n",
    "        x = self.leaky_relu2(self.batch_norm2(self.conv2(x)))\n",
    "        x = self.leaky_relu3(self.batch_norm3(self.conv3(x)))\n",
    "        x = self.leaky_relu4(self.batch_norm4(self.conv4(x)))\n",
    "        x = self.leaky_relu5(self.batch_norm5(self.conv5(x)))\n",
    "        x = self.sigmoid(self.conv6(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom weights and initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "use_weights_init = True\n",
    "\n",
    "model_G = Generator().to(device)\n",
    "if use_weights_init:\n",
    "    model_G.apply(weights_init)\n",
    "params_G = sum(p.numel() for p in model_G.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters in Generator is: {}\".format(params_G))\n",
    "print(model_G)\n",
    "print('\\n')\n",
    "\n",
    "model_D = Discriminator().to(device)\n",
    "if use_weights_init:\n",
    "    model_D.apply(weights_init)\n",
    "params_D = sum(p.numel() for p in model_D.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters in Discriminator is: {}\".format(params_D))\n",
    "print(model_D)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Total number of parameters is: {}\".format(params_G + params_D))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a loss and choose optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can modify the arguments of this function if needed\n",
    "def loss_function(out, real_or_fake):\n",
    "    if real_or_fake == 'real':\n",
    "        loss = F.binary_cross_entropy(out, torch.ones(out.size()).to(device))\n",
    "    elif real_or_fake == 'fake':\n",
    "        loss = F.binary_cross_entropy(out, torch.zeros(out.size()).to(device))\n",
    "    else:\n",
    "        raise ValueError('real_or_fake must be either \"real\" or \"fake\"')\n",
    "    return loss\n",
    "\n",
    "beta1 = 0.5\n",
    "optimizerD = torch.optim.Adam(model_D.parameters(), lr=learning_rate_d, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(model_G.parameters(), lr=learning_rate_g, betas=(beta1, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(batch_size, latent_vector_size, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_losses_G = []  # List to store generator losses\n",
    "train_losses_D = []  # List to store discriminator losses\n",
    "\n",
    "num_epochs = 300  # Total number of epochs for training\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Progress bar with tqdm to monitor each batch\n",
    "    with tqdm.tqdm(loader_train, unit=\"batch\") as tepoch: \n",
    "        train_loss_D = 0\n",
    "        train_loss_G = 0\n",
    "        for i, data in enumerate(tepoch):\n",
    "            # Initialize loss variables for this batch\n",
    "\n",
    "            \n",
    "\n",
    "            model_D.zero_grad()  \n",
    "            real_cpu = data[0].to(device)  \n",
    "            b_size = real_cpu.size(0)  \n",
    "\n",
    "\n",
    "            output = model_D(real_cpu).view(-1)\n",
    "            errD_real = loss_function(output, 'real')  # Loss for real images\n",
    "            errD_real.backward()  \n",
    "            D_x = output.mean().item() \n",
    "\n",
    "\n",
    "            noise = torch.randn(b_size, latent_vector_size, 1, 1, device=device)  # Generate random noise\n",
    "            fake = model_G(noise)  \n",
    "            output = model_D(fake.detach()).view(-1)  # Detach to avoid training G on these labels\n",
    "            errD_fake = loss_function(output, 'fake')  \n",
    "            errD_fake.backward()  \n",
    "            D_G_z1 = output.mean().item() \n",
    "            errD = errD_real + errD_fake  \n",
    "            train_loss_D += errD.item() \n",
    "            optimizerD.step()\n",
    "\n",
    "            model_G.zero_grad()  \n",
    "            output = model_D(fake).view(-1) \n",
    "            errG = loss_function(output, 'real')\n",
    "            train_loss_G += errG.item()\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Logging and updating the tqdm progress bar\n",
    "            if i % 50 == 0:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                tepoch.set_postfix(D_G_z=f\"{D_G_z1:.3f}/{D_G_z2:.3f}\", D_x=D_x,\n",
    "                                   Loss_D=errD.item(), Loss_G=errG.item())\n",
    "\n",
    "    # Save real images once at the beginning of training\n",
    "    if epoch == 0:\n",
    "        save_image(denorm(real_cpu.cpu()).float(), content_path/'CW_GAN/real_samples.png')\n",
    "    \n",
    "    # Generate and save fake images periodically\n",
    "    with torch.no_grad():\n",
    "        fake = model_G(fixed_noise)  # Generate fake images\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')  # Get current time for file naming\n",
    "        filename = str(content_path / 'CW_GAN' / f'fake_samples_epoch_{epoch:03d}_{timestamp}.png')\n",
    "        save_image(denorm(fake.cpu()).float(), filename)  # Save generated images with timestamp\n",
    "\n",
    "    \n",
    "\n",
    "    # Update training loss lists\n",
    "    train_losses_D.append(train_loss_D / len(loader_train))  # Average discriminator loss for this epoch\n",
    "    train_losses_G.append(train_loss_G / len(loader_train))  # Average generator loss for this epoch\n",
    "\n",
    "    # torch.jit.save(torch.jit.trace(model_D, (fake)), content_path/'CW_GAN/GAN_D_model.pth')\n",
    "    # torch.jit.save(torch.jit.trace(model_G, (fixed_noise)), content_path/'CW_GAN/GAN_G_model.pth')\n",
    "    loss_data = {\n",
    "        'train_loss_D': train_losses_D,\n",
    "        'train_loss_G': train_losses_G\n",
    "    }\n",
    "    filename = content_path / 'CW_GAN' / 'loss_data.pkl'\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(loss_data, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(torch.jit.trace(model_G, (fixed_noise)), content_path/'CW_GAN/GAN_G_model.pth')\n",
    "torch.jit.save(torch.jit.trace(model_D, (fake)), content_path/'CW_GAN/GAN_D_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Generator samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mG = torch.jit.load('/notebooks/CW_GAN/GAN_G_model.pth')\n",
    "model_G.load_state_dict(mG.state_dict())\n",
    "\n",
    "mD = torch.jit.load('/notebooks/CW_GAN/GAN_D_model.pth')\n",
    "model_D.load_state_dict(mD.state_dict())\n",
    "\n",
    "\n",
    "input_noise = torch.randn(100, latent_vector_size, 1, 1, device=device)\n",
    "with torch.no_grad():\n",
    "    # visualize the generated images\n",
    "    generated = model_G(input_noise).cpu()\n",
    "    generated = make_grid(denorm(generated)[:100], nrow=10, padding=2, normalize=False, \n",
    "                        value_range=None, scale_each=False, pad_value=0)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    save_image(generated, content_path/'CW_GAN/Teaching_final.png')\n",
    "    show(generated) # note these are now class conditional images columns rep classes 1-10\n",
    "\n",
    "it = iter(loader_test)\n",
    "sample_inputs, _ = next(it)\n",
    "fixed_input = sample_inputs[0:64, :, :, :]\n",
    "# visualize the original images of the last batch of the test set for comparison\n",
    "img = make_grid(denorm(fixed_input), nrow=8, padding=2, normalize=False,\n",
    "                value_range=None, scale_each=False, pad_value=0)\n",
    "plt.figure(figsize=(15,15))\n",
    "show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER FOR PART 2.2 IN THIS CELL*\n",
    "# load the loss data \n",
    "import math \n",
    "filename = content_path / 'CW_GAN' / 'loss_data.pkl'\n",
    "with open(filename, 'rb') as file:\n",
    "    loss_data = pickle.load(file)\n",
    "\n",
    "# unpack loss_data into variables\n",
    "    \n",
    "train_loss_data_D = loss_data['train_loss_D'] \n",
    "train_loss_data_G = loss_data['train_loss_G']\n",
    "\n",
    "# plot the loss data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_data_D, label='Discriminator')\n",
    "plt.plot(train_loss_data_G, label='Generator')\n",
    "plt.title('GAN Training Loss')\n",
    "\n",
    "# plot a red dashed line at x = 150\n",
    "plt.axvline(x=150, color='r', linestyle='--')\n",
    "\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')  \n",
    "\n",
    "# add a legend of the red line being the termination point \n",
    "plt.legend(['Discriminator', 'Generator', 'Termination Point'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
