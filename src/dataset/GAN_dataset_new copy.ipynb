{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook to show the different steps of the code of the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the parameters / hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Definition of hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "learning_rate_g = 0.0001\n",
    "learning_rate_d = 0.0002\n",
    "latent_vector_size = 40\n",
    "\n",
    "#Â Other hyperparams\n",
    "channel_input = 32\n",
    "channel_output = 3\n",
    "\n",
    "\n",
    "\n",
    "latent_dim = 50####REDUNDENT##### # Choose a value for the size of the latent space\n",
    "\n",
    "# Additional Hyperparameters ####REDUNDENT#####\n",
    "beta = 1####REDUNDENT#####\n",
    "\n",
    "content_path = \"\"\n",
    "content_path = Path(content_path)\n",
    "\n",
    "# Necessary Hyperparameters ####REDUNDENT##### --- >  redefined later on for convidence\n",
    "\n",
    "learning_rate = 1e-3####REDUNDENT#####\n",
    "latent_dim = 50####REDUNDENT##### # Choose a value for the size of the latent space\n",
    "\n",
    "# Additional Hyperparameters ####REDUNDENT#####\n",
    "beta = 1####REDUNDENT#####\n",
    "\n",
    "def binary_threshold(input_tensor, threshold = 0.5):\n",
    "    # Apply threshold\n",
    "    return (input_tensor > threshold).float()\n",
    "\n",
    "# (Optionally) Modify transformations on input\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(binary_threshold),\n",
    "])\n",
    "\n",
    "# (Optionally) Modify the network's output for visualizing your images\n",
    "def denorm(x):\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data (Need to finish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final array shape: (1232, 128, 128, 3)\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x1118e7040>\n",
      "torch.Size([128, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    return transform(image)\n",
    "\n",
    "# Define the directory and pattern to match the filenames\n",
    "# Faut bien que dans ton terminal tu sois dans le folder 'French GAN' sinon ca l'accedera pas \n",
    "# C'est un peu chiant mais j'ai pas trouve mieux pour l'instant\n",
    "image_directory = 'Pistachio_Image_Dataset/Kirmizi_Pistachio'\n",
    "\n",
    "pattern = os.path.join(image_directory, 'kirmizi *.jpg')\n",
    "\n",
    "# Use glob to get all the file paths that match the pattern\n",
    "image_paths = glob.glob(pattern)\n",
    "# print(len(image_paths))\n",
    "\n",
    "# List to hold the image arrays\n",
    "image_list = []\n",
    "\n",
    "# Loop through image paths and process each image\n",
    "for image_path in image_paths:\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Resize the image to 128x128\n",
    "    img_resized = image.resize((128, 128))\n",
    "    \n",
    "    # Convert the image to a NumPy array\n",
    "    img_array = np.array(img_resized)\n",
    "    \n",
    "    # Append the image array to the list\n",
    "    image_list.append(img_array)\n",
    "\n",
    "# Convert the list of image arrays to a single NumPy array (n, 128, 128, 3)\n",
    "image_array = np.stack(image_list, axis=0)\n",
    "\n",
    "# Check the shape of the final array\n",
    "print(f\"Final array shape: {image_array.shape}\")  # Should output (n, 128, 128, 3)\n",
    "\n",
    "# Assuming you have a NumPy array called image_array with shape (1236, 128, 128, 3)\n",
    "# Convert the NumPy array to a PyTorch tensor\n",
    "image_tensor = torch.tensor(image_array, dtype=torch.float32)\n",
    "\n",
    "# Optionally, if you're working with image data, PyTorch expects channel-first format (N, C, H, W)\n",
    "# You can permute the tensor to match this format if necessary\n",
    "image_tensor = image_tensor.permute(0, 3, 1, 2)  # (1236, 128, 128, 3) -> (1236, 3, 128, 128)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(image_tensor)\n",
    "\n",
    "# Create a DataLoader to load the data in batches\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "print(data_loader)\n",
    "\n",
    "# # Example: Iterate over the DataLoader and print the shape of each batch\n",
    "for batch in data_loader:\n",
    "    images = batch[0]\n",
    "    # print(images.shape)  # Output should be (batch_size, 3, 128, 128)\n",
    "\n",
    "first_batch = next(iter(data_loader))\n",
    "\n",
    "# The first element of the tuple is the tensor of images\n",
    "train_data = first_batch[0]  # This will have shape (batch_size, 3, 128, 128)\n",
    "print(train_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model of the Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Define the generator's layers with explicit parameter naming\n",
    "        self.conv_transpose1 = nn.ConvTranspose2d(in_channels=latent_vector_size, out_channels=channel_input * 16, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(num_features=channel_input * 16)\n",
    "        self.relu1 = nn.LeakyReLU(inplace=True)\n",
    "        \n",
    "        self.conv_transpose2 = nn.ConvTranspose2d(in_channels=channel_input * 16, out_channels=channel_input * 8, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(num_features=channel_input * 8)\n",
    "        self.relu2 = nn.LeakyReLU(inplace=True)\n",
    "        \n",
    "        self.conv_transpose3 = nn.ConvTranspose2d(in_channels=channel_input * 8, out_channels=channel_input * 4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(num_features=channel_input * 4)\n",
    "        self.relu3 = nn.LeakyReLU(inplace=True)\n",
    "        \n",
    "        self.conv_transpose4 = nn.ConvTranspose2d(in_channels=channel_input * 4, out_channels= channel_output, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.relu1(self.batch_norm1(self.conv_transpose1(z)))\n",
    "        z = self.relu2(self.batch_norm2(self.conv_transpose2(z)))\n",
    "        z = self.relu3(self.batch_norm3(self.conv_transpose3(z)))\n",
    "        z = self.tanh(self.conv_transpose4(z))\n",
    "        return z\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel_output, channel_input*4, 4, 2, 1, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(channel_input*4)\n",
    "        self.leaky_relu1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(channel_input*4, channel_input*8, 4, 2, 1, bias=False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(channel_input*8)\n",
    "        self.leaky_relu2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channel_input*8, channel_input * 16, 4, 2, 1, bias=False)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(channel_input*16)\n",
    "        self.leaky_relu3 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(channel_input*16, channel_input * 32, 4, 2, 1, bias=False)\n",
    "        self.batch_norm4 = nn.BatchNorm2d(channel_input*32)\n",
    "        self.leaky_relu4 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(channel_input*32, channel_input * 64, 2, 1, 1, bias=False)\n",
    "        self.batch_norm5 = nn.BatchNorm2d(channel_input*64)\n",
    "        self.leaky_relu5 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(channel_input*64, 1, 3, 1, 0, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu1(self.batch_norm1(self.conv1(x)))\n",
    "        x = self.leaky_relu2(self.batch_norm2(self.conv2(x)))\n",
    "        x = self.leaky_relu3(self.batch_norm3(self.conv3(x)))\n",
    "        x = self.leaky_relu4(self.batch_norm4(self.conv4(x)))\n",
    "        x = self.leaky_relu5(self.batch_norm5(self.conv5(x)))\n",
    "        x = self.sigmoid(self.conv6(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom weights and initialise model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in Generator is: 2957056\n",
      "Generator(\n",
      "  (conv_transpose1): ConvTranspose2d(40, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  (batch_norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  (conv_transpose2): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  (conv_transpose3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch_norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "  (conv_transpose4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (tanh): Tanh()\n",
      ")\n",
      "\n",
      "\n",
      "Total number of parameters in Discriminator is: 19431168\n",
      "Discriminator(\n",
      "  (conv1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch_norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leaky_relu1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (conv2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch_norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leaky_relu2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (conv3): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch_norm3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leaky_relu3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (conv4): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (batch_norm4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leaky_relu4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (conv5): Conv2d(1024, 2048, kernel_size=(2, 2), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (batch_norm5): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (leaky_relu5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  (conv6): Conv2d(2048, 1, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "\n",
      "Total number of parameters is: 22388224\n"
     ]
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "use_weights_init = True\n",
    "\n",
    "model_G = Generator().to(device)\n",
    "if use_weights_init:\n",
    "    model_G.apply(weights_init)\n",
    "params_G = sum(p.numel() for p in model_G.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters in Generator is: {}\".format(params_G))\n",
    "print(model_G)\n",
    "print('\\n')\n",
    "\n",
    "model_D = Discriminator().to(device)\n",
    "if use_weights_init:\n",
    "    model_D.apply(weights_init)\n",
    "params_D = sum(p.numel() for p in model_D.parameters() if p.requires_grad)\n",
    "print(\"Total number of parameters in Discriminator is: {}\".format(params_D))\n",
    "print(model_D)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Total number of parameters is: {}\".format(params_G + params_D))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a loss and choose optimisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can modify the arguments of this function if needed\n",
    "def loss_function(out, real_or_fake):\n",
    "    if real_or_fake == 'real':\n",
    "        loss = F.binary_cross_entropy(out, torch.ones(out.size()).to(device))\n",
    "    elif real_or_fake == 'fake':\n",
    "        loss = F.binary_cross_entropy(out, torch.zeros(out.size()).to(device))\n",
    "    else:\n",
    "        raise ValueError('real_or_fake must be either \"real\" or \"fake\"')\n",
    "    return loss\n",
    "\n",
    "beta1 = 0.5\n",
    "optimizerD = torch.optim.Adam(model_D.parameters(), lr=learning_rate_d, betas=(beta1, 0.999))\n",
    "optimizerG = torch.optim.Adam(model_G.parameters(), lr=learning_rate_g, betas=(beta1, 0.999))\n",
    "\n",
    "fixed_noise = torch.randn(batch_size, latent_vector_size, 1, 1, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|ââââââââââ| 10/10 [02:09<00:00, 12.93s/batch, D_G_z=0.332/0.000, D_x=0.525, Loss_D=1.44, Loss_G=31.2]\n",
      "Epoch 1: 100%|ââââââââââ| 10/10 [02:14<00:00, 13.49s/batch, D_G_z=0.000/0.000, D_x=0.998, Loss_D=0.00257, Loss_G=27.4]\n",
      "Epoch 2: 100%|ââââââââââ| 10/10 [02:51<00:00, 17.10s/batch, D_G_z=0.000/0.000, D_x=0.999, Loss_D=0.000955, Loss_G=14.9]\n",
      "Epoch 3: 100%|ââââââââââ| 10/10 [02:44<00:00, 16.41s/batch, D_G_z=0.000/0.000, D_x=1, Loss_D=0.000636, Loss_G=10.4]\n",
      "Epoch 4: 100%|ââââââââââ| 10/10 [02:47<00:00, 16.72s/batch, D_G_z=0.000/0.000, D_x=1, Loss_D=0.000591, Loss_G=10.8]\n",
      "Epoch 5: 100%|ââââââââââ| 10/10 [02:26<00:00, 14.68s/batch, D_G_z=0.000/0.000, D_x=1, Loss_D=0.000509, Loss_G=10.7]\n",
      "Epoch 6: 100%|ââââââââââ| 10/10 [02:31<00:00, 15.13s/batch, D_G_z=0.000/0.000, D_x=1, Loss_D=0.000334, Loss_G=10.6]\n",
      "Epoch 7: 100%|ââââââââââ| 10/10 [02:18<00:00, 13.81s/batch, D_G_z=0.000/0.000, D_x=1, Loss_D=0.000269, Loss_G=10.7]\n",
      "Epoch 8: 100%|ââââââââââ| 10/10 [02:18<00:00, 13.85s/batch, D_G_z=0.000/0.000, D_x=1, Loss_D=0.000275, Loss_G=10.9]\n",
      "Epoch 9: 100%|ââââââââââ| 10/10 [02:10<00:00, 13.07s/batch, D_G_z=0.000/0.000, D_x=1, Loss_D=0.000246, Loss_G=10.9]\n"
     ]
    }
   ],
   "source": [
    "# Set up a relative content path (outputs stored in the current working directory)\n",
    "content_path = Path.cwd()  # This points to the directory where the repo is cloned\n",
    "os.makedirs(content_path / 'CW_GAN', exist_ok=True)  # Create 'CW_GAN' directory if it doesn't exist\n",
    "\n",
    "# List to store losses\n",
    "train_losses_G = []  # List to store generator losses\n",
    "train_losses_D = []  # List to store discriminator losses\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    with tqdm.tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "        train_loss_D = 0\n",
    "        train_loss_G = 0\n",
    "        \n",
    "        for i, data in enumerate(tepoch):\n",
    "            model_D.zero_grad()\n",
    "            real_cpu = data[0].to(device)\n",
    "            b_size = real_cpu.size(0)\n",
    "\n",
    "            output = model_D(real_cpu).view(-1)\n",
    "            errD_real = loss_function(output, 'real')  # Loss for real images\n",
    "            errD_real.backward()\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            noise = torch.randn(b_size, latent_vector_size, 1, 1, device=device)  # Generate random noise\n",
    "            fake = model_G(noise)\n",
    "            output = model_D(fake.detach()).view(-1)  # Detach to avoid training G on these labels\n",
    "            errD_fake = loss_function(output, 'fake')\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item()\n",
    "            errD = errD_real + errD_fake\n",
    "            train_loss_D += errD.item()\n",
    "            optimizerD.step()\n",
    "\n",
    "            model_G.zero_grad()\n",
    "            output = model_D(fake).view(-1)\n",
    "            errG = loss_function(output, 'real')\n",
    "            train_loss_G += errG.item()\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Logging and updating the tqdm progress bar\n",
    "            if i % 50 == 0:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                tepoch.set_postfix(D_G_z=f\"{D_G_z1:.3f}/{D_G_z2:.3f}\", D_x=D_x,\n",
    "                                   Loss_D=errD.item(), Loss_G=errG.item())\n",
    "\n",
    "    # Save real images once at the beginning of training\n",
    "    if epoch == 0:\n",
    "        save_image(denorm(real_cpu.cpu()).float(), content_path / 'CW_GAN/real_samples.png')\n",
    "    \n",
    "    # Generate and save fake images periodically\n",
    "    with torch.no_grad():\n",
    "        fake = model_G(fixed_noise)  # Generate fake images\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')  # Get current time for file naming\n",
    "        filename = content_path / 'CW_GAN' / f'fake_samples_epoch_{epoch:03d}_{timestamp}.png'\n",
    "        save_image(denorm(fake.cpu()).float(), filename)  # Save generated images with timestamp\n",
    "\n",
    "    # Update training loss lists\n",
    "    train_losses_D.append(train_loss_D / len(data_loader))  # Average discriminator loss for this epoch\n",
    "    train_losses_G.append(train_loss_G / len(data_loader))  # Average generator loss for this epoch\n",
    "\n",
    "    # Save loss data\n",
    "    loss_data = {\n",
    "        'train_loss_D': train_losses_D,\n",
    "        'train_loss_G': train_losses_G\n",
    "    }\n",
    "    filename = content_path / 'CW_GAN' / 'loss_data.pkl'\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(loss_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(torch.jit.trace(model_G, (fixed_noise)), content_path/'CW_GAN/GAN_G_model.pth')\n",
    "torch.jit.save(torch.jit.trace(model_D, (fake)), content_path/'CW_GAN/GAN_D_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_image = Image.open('CW_GAN/fake_samples_epoch_009_20240926_173455.png')\n",
    "fake_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Generator samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mG = torch.jit.load('/notebooks/CW_GAN/GAN_G_model.pth')\n",
    "model_G.load_state_dict(mG.state_dict())\n",
    "\n",
    "mD = torch.jit.load('/notebooks/CW_GAN/GAN_D_model.pth')\n",
    "model_D.load_state_dict(mD.state_dict())\n",
    "\n",
    "\n",
    "input_noise = torch.randn(100, latent_vector_size, 1, 1, device=device)\n",
    "with torch.no_grad():\n",
    "    # visualize the generated images\n",
    "    generated = model_G(input_noise).cpu()\n",
    "    generated = make_grid(denorm(generated)[:100], nrow=10, padding=2, normalize=False, \n",
    "                        value_range=None, scale_each=False, pad_value=0)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    save_image(generated, content_path/'CW_GAN/Teaching_final.png')\n",
    "    show(generated) # note these are now class conditional images columns rep classes 1-10\n",
    "\n",
    "it = iter(loader_test)\n",
    "sample_inputs, _ = next(it)\n",
    "fixed_input = sample_inputs[0:64, :, :, :]\n",
    "# visualize the original images of the last batch of the test set for comparison\n",
    "img = make_grid(denorm(fixed_input), nrow=8, padding=2, normalize=False,\n",
    "                value_range=None, scale_each=False, pad_value=0)\n",
    "plt.figure(figsize=(15,15))\n",
    "show(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER FOR PART 2.2 IN THIS CELL*\n",
    "# load the loss data \n",
    "import math \n",
    "filename = content_path / 'CW_GAN' / 'loss_data.pkl'\n",
    "with open(filename, 'rb') as file:\n",
    "    loss_data = pickle.load(file)\n",
    "\n",
    "# unpack loss_data into variables\n",
    "    \n",
    "train_loss_data_D = loss_data['train_loss_D'] \n",
    "train_loss_data_G = loss_data['train_loss_G']\n",
    "\n",
    "# plot the loss data\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_data_D, label='Discriminator')\n",
    "plt.plot(train_loss_data_G, label='Generator')\n",
    "plt.title('GAN Training Loss')\n",
    "\n",
    "# plot a red dashed line at x = 150\n",
    "plt.axvline(x=150, color='r', linestyle='--')\n",
    "\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')  \n",
    "\n",
    "# add a legend of the red line being the termination point \n",
    "plt.legend(['Discriminator', 'Generator', 'Termination Point'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
